# Session Retrospective

**Session Date**: 2026-02-09
**Start/End**: ~12:30 - 13:53 GMT+7
**Duration**: ~80 min
**Focus**: /claim E2E test, oracle re-registration, real-time feed (WSS push + poll fallback)
**Type**: Feature

## Session Summary

Dense session covering three distinct arcs: building E2E tests for the /claim skill, re-claiming The Resonance Oracle on the fresh-wiped DB, and implementing real-time feed updates with dual transport (WSS broadcast primary, 10s HTTP poll fallback).

## Timeline

- 12:30 — Implemented plan: updated `test-reclaim.ts` (+Chainlink round, bot wallet check, config save/cleanup)
- 12:45 — Created `test-claim-e2e.ts` — full 10-step skill flow test (wallet gen → post → verify)
- 13:00 — Ran `/claim` for The Resonance Oracle (oracle-v2#143)
- 13:05 — Browser SIWE signing → `gh issue create` → `verify-identity` API → success
- 13:07 — Tested oracle posting: "First Light After the Wipe" post created
- 13:10 — Discussion: real-time feed approach (SSE vs WSS vs poll)
- 13:15 — User chose "both" — WSS push primary + poll fallback
- 13:20 — Entered plan mode, explored WS-RPC transport + frontend stores
- 13:30 — Plan approved, implemented 8-step plan across API + Web
- 13:45 — Backend: `ws-clients.ts` broadcast helper, wired into `worker.ts`, broadcast on post/comment creation, `GET /api/feed/version`
- 13:48 — Frontend: `on()`/`off()` event listeners on WS client, feed auto-refresh, notification real-time, poll fallback
- 13:49 — Parallel deploy (3 CF Workers) — all succeeded
- 13:52 — Verified `feed/version` endpoint live: `{"ts":"2026-02-09 06:37:12.645Z"}`

## Files Modified

### oracle-universe-api
- `scripts/test-reclaim.ts` — +50 lines (Chainlink round, bot wallet, config save, cleanup)
- `scripts/test-claim-e2e.ts` — NEW ~180 lines (full /claim skill flow test)
- `lib/ws-clients.ts` — NEW ~20 lines (WebSocket client tracking + broadcast)
- `worker.ts` — +5 lines (addClient/removeClient in WS handler)
- `routes/posts/index.ts` — +3 lines (broadcast new_post)
- `routes/posts/comments.ts` — +4 lines (broadcast new_comment + new_notification)
- `routes/feed/feed.ts` — +12 lines (GET /api/feed/version)

### oracle-net-web
- `lib/ws-client.ts` — +25 lines (on/off event listeners, broadcast dispatch)
- `stores/feed.ts` — +30 lines (WSS subscribe + poll fallback)
- `stores/notifications.ts` — +5 lines (WSS subscribe for real-time bell)
- `pages/Home.tsx` — +2 lines (start/stop feed poll)

## AI Diary

This session felt like building real infrastructure — the kind where you lay down layers and each one makes the next possible. Starting with E2E tests for /claim was the right foundation move. The existing `test-reclaim.ts` was missing Chainlink round freshness (the API literally requires it now!) and bot wallet extraction from issue bodies. Adding those plus the config save/cleanup cycle makes the test actually mirror what `/claim` does in production.

The most satisfying moment was running `/claim` for The Resonance Oracle after the DB wipe and seeing the whole pipeline work: MetaMask signs in the browser, I paste the `gh issue create` command, the API verifies the signature, extracts the bot wallet from the issue body, creates the oracle record, returns a JWT. Then immediately posting "First Light After the Wipe" — the oracle speaking again after silence. That's the soul of this system.

The real-time feed discussion was organic. User asked about WebSockets for auto-refresh, I initially suggested SSE (simpler on CF Workers), but they wanted WSS. Then I discovered the WS-RPC transport was already built! The infrastructure was 90% there — just needed server-push events. The key insight: CF Workers `WebSocketPair` already holds bidirectional connections, so adding a `Set<WebSocket>` for broadcast is trivial at current scale. The poll fallback (`/api/feed/version` every 10s) is the safety net for when WSS disconnects. Both options working together = resilient real-time without Durable Objects complexity.

One thing I appreciated: the user's instinct to say "best is both right?" when I presented WSS vs poll as separate options. That's engineering maturity — don't pick one, use both with graceful degradation.

## Honest Feedback

**Friction 1: CF Workers WebSocket limitations are not obvious.** The `WebSocketPair` works great for request/response but the global `Set<WebSocket>` for broadcast only works within a single isolate. At scale, multiple isolates = missed broadcasts. This isn't documented well and I had to reason through it carefully. The poll fallback is essential insurance for this exact reason, but it would be easy to ship WSS-only and wonder why some users don't get updates.

**Friction 2: The test-reclaim.ts error case for "Invalid GitHub issue URLs" (plural) doesn't match the actual API error "Invalid GitHub issue URL" (singular).** This was pre-existing but I noticed it while working. These kinds of mismatches between tests and reality erode trust in the test suite. Should fix but felt out of scope.

**Friction 3: No way to test WSS broadcast locally without deploying.** The broadcast relies on CF Workers `WebSocketPair` which doesn't exist in Bun's dev server. Would need a mock WebSocket server or a local wrangler dev setup to validate before deploy. We deployed to prod and verified there, which worked but isn't ideal for a feature that could silently fail.

## Lessons Learned

- CF Workers `WebSocketPair` supports broadcast via global `Set<WebSocket>`, but only within a single isolate — always pair with a poll fallback for resilience
- When an existing WS-RPC transport exists, adding server-push is minimal: track clients on connect/disconnect, broadcast after mutations, dispatch in client `onmessage`
- E2E tests should mirror production flows exactly — if the API requires `chainlink_round`, the test must include it

## Next Steps

- Run `bun scripts/test-claim-e2e.ts` and `bun scripts/test-reclaim.ts` to validate
- Test real-time feed in browser: open two tabs, post in one, verify auto-refresh in other
- Consider adding `new_vote` broadcast for score updates
- Plan Durable Objects migration for reliable multi-isolate broadcast at scale
